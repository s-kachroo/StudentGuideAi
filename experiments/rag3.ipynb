{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bdbbfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip -q\n",
    "%pip install -r ../requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "926719a487baa87b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T19:11:49.805145Z",
     "start_time": "2025-04-07T19:11:49.798948Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from textwrap import dedent\n",
    "from torchmetrics.text.bert import BERTScore\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"tqdm\")\n",
    "\n",
    "# Set up OpenAI API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e83b078f005502",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T19:11:49.828668Z",
     "start_time": "2025-04-07T19:11:49.825813Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_pdf_documents(data_dir):\n",
    "    \"\"\"\n",
    "    Loads all PDF files from the specified directory.\n",
    "    Returns a list of document dictionaries.\n",
    "    \"\"\"\n",
    "    documents_list = []\n",
    "    data_path = Path(data_dir)\n",
    "    if not data_path.exists():\n",
    "        raise FileNotFoundError(f\"Data directory not found at {data_path.absolute()}\")\n",
    "    pdf_files = list(data_path.glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(f\"No PDF files found in {data_path.absolute()}\")\n",
    "    for pdf_file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "        try:\n",
    "            text = \"\"\n",
    "            with open(pdf_file, \"rb\") as f:\n",
    "                reader = PyPDF2.PdfReader(f)\n",
    "                for page in reader.pages:\n",
    "                    # Use a fallback in case extract_text() returns None\n",
    "                    page_text = page.extract_text() or \"\"\n",
    "                    text += page_text + \"\\n\"\n",
    "            documents_list.append({\n",
    "                \"filepath\": str(pdf_file.absolute()),\n",
    "                \"filename\": pdf_file.name,\n",
    "                \"title\": pdf_file.stem,\n",
    "                \"text\": text,\n",
    "                \"source\": \"CMU Official Documents\"\n",
    "            })\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {pdf_file.name}: {str(e)}\")\n",
    "            continue\n",
    "    return documents_list\n",
    "\n",
    "def clean_document_text(text):\n",
    "    \"\"\"\n",
    "    Cleans text by removing extra spaces and some common unwanted patterns.\n",
    "    \"\"\"\n",
    "    text = \" \".join(text.split())\n",
    "    patterns = [r\"page \\d+ of \\d+\", r\"confidential\", r\"Â©\\d+\"]\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, \"\", text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "def chunk_documents(documents_data, chunk_size, chunk_overlap):\n",
    "    \"\"\"\n",
    "    Splits each document's text into chunks (based on words).\n",
    "    Optionally cleans the text before splitting.\n",
    "    Returns a list of chunk dictionaries.\n",
    "    \"\"\"\n",
    "    chunks_list = []\n",
    "    for doc in tqdm(documents_data, desc=\"Chunking documents\"):\n",
    "        text = doc[\"text\"]\n",
    "        text = clean_document_text(text=text)\n",
    "        words = text.split()\n",
    "        # Slide a window over the word list\n",
    "        for i in range(0, len(words), chunk_size - chunk_overlap):\n",
    "            chunk_words = words[i: i + chunk_size]\n",
    "            chunk_text = \" \".join(chunk_words)\n",
    "            chunks_list.append({\n",
    "                \"text\": chunk_text,\n",
    "                \"document_title\": doc[\"title\"],\n",
    "                \"document_source\": doc[\"source\"],\n",
    "                \"chunk_id\": f\"{doc['title']}_{len(chunk_text)}_{hash(chunk_text)}\",\n",
    "                \"metadata\": {\n",
    "                    \"source\": doc[\"source\"],\n",
    "                    \"title\": doc[\"title\"],\n",
    "                    \"filepath\": doc[\"filepath\"]\n",
    "                }\n",
    "            })\n",
    "    return chunks_list\n",
    "\n",
    "def generate_questions_for_chunk(chunk, num_questions, model_name):\n",
    "    \"\"\"\n",
    "    Given one chunk dict (with keys 'text' and 'chunk_id'),\n",
    "    generate `num_questions` that can be answered from it.\n",
    "    Returns a list of question strings.\n",
    "    \"\"\"\n",
    "    prompt = dedent(f\"\"\"\n",
    "        You are given the following excerpt from a CMU policy document:\n",
    "        \\\"\\\"\\\"{chunk['text']}\\\"\\\"\\\"\n",
    "\n",
    "        Generate {num_questions} clear, concise questions that can be answered\n",
    "        using ONLY this excerpt. Return them as:\n",
    "        Q1: ...\n",
    "        Q2: ...\n",
    "        Q3: ...\n",
    "    \"\"\").strip()\n",
    "\n",
    "    client = OpenAI(api_key=openai.api_key)\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3\n",
    "    )\n",
    "\n",
    "    questions = []\n",
    "    for line in resp.choices[0].message.content.splitlines():\n",
    "        m = re.match(r\"Q\\d+:\\s*(.+)\", line.strip())\n",
    "        if m:\n",
    "            questions.append(m.group(1).strip())\n",
    "    time.sleep(0.05)\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a8365def3b67c86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T19:11:49.837484Z",
     "start_time": "2025-04-07T19:11:49.834602Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_default_vector_collection(build=False, chunks=None, collection_name=\"cmu_student_guide\", db_path=\"./chroma_db\"):\n",
    "    \"\"\"\n",
    "    Returns a persistent ChromaDB collection.\n",
    "    If build is set to True, PDFs are processed, chunked, and added to the collection;\n",
    "    otherwise, the saved vector database (in ./chroma_db) is loaded.\n",
    "    \"\"\"\n",
    "    client = chromadb.PersistentClient(\n",
    "        path=db_path,\n",
    "        settings=Settings(anonymized_telemetry=False)\n",
    "    )\n",
    "    rag_collection = client.get_or_create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=embedding_functions.SentenceTransformerEmbeddingFunction(\"all-MiniLM-L6-v2\")\n",
    "    )\n",
    "    if build:\n",
    "        for i in tqdm(range(0, len(chunks), 100), desc=\"Indexing documents\"):\n",
    "            batch = chunks[i: i + 100]\n",
    "            rag_collection.add(\n",
    "                documents=[chunk[\"text\"] for chunk in batch],\n",
    "                metadatas=[chunk[\"metadata\"] for chunk in batch],\n",
    "                ids=[chunk[\"chunk_id\"] for chunk in batch]\n",
    "            )\n",
    "    return rag_collection\n",
    "\n",
    "def create_question_vector_collection(build=False, chunks=None, collection_name=\"cmu_question_index\", db_path=\"./chroma_qdb\", num_questions=5):\n",
    "    \"\"\"\n",
    "    Build or load a ChromaDB collection whose 'documents' are\n",
    "    LL-generated questions, each tagged with chunk_id metadata.\n",
    "    \"\"\"\n",
    "    client = chromadb.PersistentClient(\n",
    "        path=db_path,\n",
    "        settings=Settings(anonymized_telemetry=False)\n",
    "    )\n",
    "    qcol = client.get_or_create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=embedding_functions.SentenceTransformerEmbeddingFunction(\"all-MiniLM-L6-v2\")\n",
    "    )\n",
    "    if build:\n",
    "        for chunk in tqdm(chunks, desc=\"Generating & indexing questions\"):\n",
    "            qs = generate_questions_for_chunk(chunk=chunk, num_questions=num_questions, model_name=\"gpt-4o-mini\")\n",
    "            for idx, q in enumerate(qs):\n",
    "                qid = f\"{chunk['chunk_id']}_q{idx}\"\n",
    "                qcol.add(\n",
    "                    documents=[q],\n",
    "                    metadatas=[{\"chunk_id\": chunk[\"chunk_id\"]}],\n",
    "                    ids=[qid]\n",
    "                )\n",
    "    return qcol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dace1ee29ecb9f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T19:11:49.863543Z",
     "start_time": "2025-04-07T19:11:49.861397Z"
    }
   },
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(rag_collection, query, top_k):\n",
    "    \"\"\"\n",
    "    Queries the vector DB for the most similar document chunks to the query.\n",
    "    Returns the raw query results.\n",
    "    \"\"\"\n",
    "    results = rag_collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=top_k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    results[\"scores\"] = [1 - distance for distance in results[\"distances\"][0]]\n",
    "    return results\n",
    "\n",
    "def retrieve_relevant_chunks_via_questions(qcol, chunks, query, top_k):\n",
    "    \"\"\"\n",
    "    1) Query the question index for the top_k most similar LL-generated questions.\n",
    "    2) Collect the unique chunk_ids from their metadata.\n",
    "    3) Return the corresponding chunk dicts.\n",
    "    \"\"\"\n",
    "    res = qcol.query(\n",
    "        query_texts=[query],\n",
    "        n_results=top_k,\n",
    "        include=[\"metadatas\"]\n",
    "    )\n",
    "    ids = {m[\"chunk_id\"] for m in res[\"metadatas\"][0]}\n",
    "    return [c for c in chunks if c[\"chunk_id\"] in ids]\n",
    "\n",
    "def generate_answer(query, retrieved_chunks, model_name=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Generates an answer by constructing a prompt that includes the retrieved context\n",
    "    and then calling the OpenAI ChatCompletion API.\n",
    "    \"\"\"\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"Source: {meta['title']}\\n{doc}\"\n",
    "        for doc, meta in zip(retrieved_chunks[\"documents\"][0], retrieved_chunks[\"metadatas\"][0])\n",
    "    ])\n",
    "\n",
    "    prompt = dedent(f\"\"\"\n",
    "        You are a helpful CMU assistant. Answer based ONLY on this context:\n",
    "        {context}\n",
    "        Question: {query}\n",
    "        Please give concise answer with less than 15 words, meaningful, and cite sources, if possible.\n",
    "        \"\"\")\n",
    "\n",
    "    client = OpenAI(api_key=openai.api_key)\n",
    "    llm_response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a factual CMU student assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return llm_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb66dff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_cmu_knowledge(user_question, use_questions=False, d_collection=None, q_collection=None, chunks=None, top_k=3):\n",
    "    \"\"\"\n",
    "    Retrieves document context using the vector DB and generates a final answer.\n",
    "    Returns a dictionary with the question, answer, and source metadata.\n",
    "    \"\"\"    \n",
    "    try:\n",
    "        if use_questions:\n",
    "            assert q_collection and chunks, \"need question_col and chunks for use_questions=True\"\n",
    "            selected_chunks = retrieve_relevant_chunks_via_questions(qcol=q_collection, chunks=chunks, query=user_question, top_k=top_k)\n",
    "            retrieved = {\"documents\": [[c[\"text\"] for c in selected_chunks]], \"metadatas\": [[c[\"metadata\"] for c in selected_chunks]]}\n",
    "        else:\n",
    "            retrieved = retrieve_relevant_chunks(rag_collection=d_collection, query=user_question, top_k=top_k)\n",
    "\n",
    "        answer = generate_answer(query=user_question, retrieved_chunks=retrieved)\n",
    "        return {\n",
    "            \"question\": user_question,\n",
    "            \"answer\": answer,\n",
    "            \"sources\": retrieved[\"metadatas\"][0]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Query failed: {e}\")\n",
    "        return {\n",
    "            \"question\": user_question,\n",
    "            \"answer\": \"Sorry, I couldn't process your question. Please contact The HUB.\",\n",
    "            \"sources\": []\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fdf9c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|ââââââââââ| 10/10 [00:00<00:00, 23.99it/s]\n",
      "Chunking documents: 100%|ââââââââââ| 10/10 [00:00<00:00, 4269.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Building the default vector database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing documents: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Built the vector database with 12 chunks.\n",
      "\n",
      "[INFO] Building the question index vector database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating & indexing questions: 100%|ââââââââââ| 12/12 [01:13<00:00,  6.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Built the question index with 60 questions.\n"
     ]
    }
   ],
   "source": [
    "# Load the CMU student guide PDF documents and chunk them\n",
    "data_dir = \"../data\"\n",
    "documents = load_pdf_documents(data_dir=data_dir)\n",
    "chunks = chunk_documents(documents_data=documents, chunk_size=1500, chunk_overlap=100)\n",
    "\n",
    "# Build the default vector database.\n",
    "print(f\"\\n[INFO] Building the default vector database...\")\n",
    "collection_default = create_default_vector_collection(build=True, chunks=chunks, collection_name=\"cmu_student_guide\", db_path=\"./chroma_db\")\n",
    "print(f\"[INFO] Built the vector database with {len(collection_default.get()['ids'])} chunks.\")\n",
    "\n",
    "# Build the question index vector database.\n",
    "print(f\"\\n[INFO] Building the question index vector database...\")\n",
    "collection_question = create_question_vector_collection(build=True, chunks=chunks, collection_name=\"cmu_question_index\", db_path=\"./chroma_qdb\")\n",
    "print(f\"[INFO] Built the question index with {len(collection_question.get()['ids'])} questions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ce4b685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing RAG via default config ---\n",
      "Question: What is the deadline to add a course?, Answer: The deadline to add a course is not specified in the provided context.\n",
      "\n",
      "--- Testing RAG via question-index config ---\n",
      "Question : What is the deadline to add a course?, Answer: The deadline to add a course is February 15.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Testing RAG via default config ---\")\n",
    "question1 = \"What is the deadline to add a course?\"\n",
    "response1 = query_cmu_knowledge(user_question=question1, use_questions=False, d_collection=collection_default, chunks=chunks, top_k=3)\n",
    "print(f\"Question: {question1}, Answer: {response1['answer']}\")\n",
    "\n",
    "print(\"\\n--- Testing RAG via question-index config ---\")\n",
    "question2 = \"What is the deadline to add a course?\"\n",
    "response2 = query_cmu_knowledge(user_question=question2, use_questions=True, q_collection=collection_question, chunks=chunks, top_k=3)\n",
    "print(f\"Question : {question2}, Answer: {response2['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4ab29bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running evaluation with use_questions=False...\n",
      "Evaluating: 'How do I access library resources?'\n",
      "Evaluating: 'Who is Cathleen Kisak?'\n",
      "--> Average Precision=0.5006, Recall=0.6128, F1=0.5504\n",
      "[{'question': 'How do I access library resources?', 'generated': 'Access library resources through the CMU library website: http://www.cmu.edu.', 'reference': 'Use your Andrew ID at the library website.'}, {'question': 'Who is Cathleen Kisak?', 'generated': 'Cathleen Kisak is a Research Designer and Analyst at Institutional Research and Analysis. (Source: cds-2024-a-general-information)', 'reference': 'Cathleen Kisak is a faculty member at Carnegie Mellon University, known for her role in the School of Computer Science.'}]\n",
      "\n",
      "Running evaluation with use_questions=True...\n",
      "Evaluating: 'How do I access library resources?'\n",
      "Evaluating: 'Who is Cathleen Kisak?'\n",
      "--> Average Precision=0.7264, Recall=0.6722, F1=0.6959\n",
      "[{'question': 'How do I access library resources?', 'generated': 'Access library resources through the CMU library website or campus portal.', 'reference': 'Use your Andrew ID at the library website.'}, {'question': 'Who is Cathleen Kisak?', 'generated': 'Cathleen Kisak is a Research Designer and Analyst at Carnegie Mellon University.', 'reference': 'Cathleen Kisak is a faculty member at Carnegie Mellon University, known for her role in the School of Computer Science.'}]\n"
     ]
    }
   ],
   "source": [
    "# Create the BERTScore metric for evaluation\n",
    "bertscore_scorer = BERTScore(\n",
    "    model_name_or_path=\"bert-base-uncased\",\n",
    "    num_layers=8,\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "# Function to evaluate the generated answer using BERTScore\n",
    "def evaluate_response(generated_answer, reference_answer):\n",
    "    scores = bertscore_scorer(\n",
    "        [generated_answer],\n",
    "        [reference_answer]\n",
    "    )\n",
    "    p = scores[\"precision\"].item()\n",
    "    r = scores[\"recall\"].item()\n",
    "    f = scores[\"f1\"].item()\n",
    "    return p, r, f\n",
    "\n",
    "# Function to run the evaluation on a set of test cases\n",
    "def run_evaluation(test_user_queries, use_questions=False, d_collection=None, q_collection=None, chunks=None, top_k=3):\n",
    "    \"\"\"\n",
    "    Evaluate your RAG pipeline over test_user_queries (list of {\"question\",\"answer\"}).\n",
    "    If use_questions=True, routes through the question-index pipeline.\n",
    "    Otherwise uses the default d_collection retrieval.\n",
    "    Prints average Precision/Recall/F1 (BERTScore) and returns the per-query results.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    ps, rs, fs = [], [], []\n",
    "    print(f\"\\nRunning evaluation with use_questions={use_questions}...\")\n",
    "    for case in test_user_queries:\n",
    "        q = case[\"question\"]\n",
    "        print(f\"Evaluating: {q!r}\")\n",
    "        resp = query_cmu_knowledge(user_question=q, use_questions=use_questions, d_collection=d_collection, q_collection=q_collection, chunks=chunks, top_k=top_k)\n",
    "        p, r, f = evaluate_response(generated_answer=resp[\"answer\"], reference_answer=case[\"answer\"])\n",
    "        ps.append(p); rs.append(r); fs.append(f)\n",
    "        results.append({\"question\": q, \"generated\": resp[\"answer\"], \"reference\": case[\"answer\"]})\n",
    "    avg_p = sum(ps) / len(ps)\n",
    "    avg_r = sum(rs) / len(rs)\n",
    "    avg_f = sum(fs) / len(fs)\n",
    "    print(f\"--> Average Precision={avg_p:.4f}, Recall={avg_r:.4f}, F1={avg_f:.4f}\")\n",
    "    return results\n",
    "\n",
    "# Run evaluation on a set of test cases.\n",
    "test_user_queries = [\n",
    "    {\n",
    "        \"question\": \"How do I access library resources?\",\n",
    "        \"answer\": \"Use your Andrew ID at the library website.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Who is Cathleen Kisak?\",\n",
    "        \"answer\": \"Cathleen Kisak is a faculty member at Carnegie Mellon University, known for her role in the School of Computer Science.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 1. Default RAG evaluation\n",
    "df_default_rag_evalutaion   = run_evaluation(test_user_queries=test_user_queries, use_questions=False, d_collection=collection_default, q_collection=None, chunks=chunks, top_k=3)\n",
    "print(df_default_rag_evalutaion)\n",
    "\n",
    "# 2. Question-index RAG evaluation\n",
    "df_questions_rag_evalutaion = run_evaluation(test_user_queries=test_user_queries, use_questions=True, d_collection=None, q_collection=collection_question, chunks=chunks, top_k=3)\n",
    "print(df_questions_rag_evalutaion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
