{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "926719a487baa87b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T19:11:49.805145Z",
     "start_time": "2025-04-07T19:11:49.798948Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "from tqdm import tqdm\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from textwrap import dedent\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3307554d703734c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T19:11:49.819412Z",
     "start_time": "2025-04-07T19:11:49.815104Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to load PDF documents from a specified directory\n",
    "def load_pdf_documents(data_dir=\"./data\"):\n",
    "    documents = []\n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    # Check if the data directory exists\n",
    "    if not data_path.exists():\n",
    "        raise FileNotFoundError(f\"Data directory not found at {data_path.absolute()}\")\n",
    "    \n",
    "    # Get all PDF files in the directory\n",
    "    pdf_files = list(data_path.glob(\"*.pdf\"))\n",
    "    \n",
    "    # Check if any PDF files were found\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(f\"No PDF files found in {data_path.absolute()}\")\n",
    "\n",
    "    # Process each PDF file\n",
    "    for pdf_file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "        try:\n",
    "            text = \"\"\n",
    "            with open(pdf_file, 'rb') as f:\n",
    "                reader = PyPDF2.PdfReader(f)\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "\n",
    "            documents.append({\n",
    "                'filepath': str(pdf_file.absolute()),\n",
    "                'filename': pdf_file.name,\n",
    "                'title': pdf_file.stem,\n",
    "                'text': text,\n",
    "                'source': 'CMU Official Documents'\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {pdf_file.name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return documents\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def clean_document_text(text):\n",
    "    text = ' '.join(text.split())\n",
    "    patterns = [r'page \\d+ of \\d+', r'confidential', r'Â©\\d+']\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e83b078f005502",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T19:11:49.828668Z",
     "start_time": "2025-04-07T19:11:49.825813Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to chunk documents into smaller pieces\n",
    "def chunk_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    chunks = []\n",
    "    for doc in tqdm(documents, desc=\"Chunking documents\"):\n",
    "        text = doc['text']\n",
    "        words = text.split()\n",
    "        for i in range(0, len(words), chunk_size - chunk_overlap):\n",
    "            chunk_words = words[i:i + chunk_size]\n",
    "            chunk_text = ' '.join(chunk_words)\n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'document_title': doc['title'],\n",
    "                'document_source': doc['source'],\n",
    "                'chunk_id': f\"{doc['title']}_{len(chunk_text)}_{hash(chunk_text)}\",\n",
    "                'metadata': {\n",
    "                    'source': doc['source'],\n",
    "                    'title': doc['title'],\n",
    "                    'filepath': doc['filepath']\n",
    "                }\n",
    "            })\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a8365def3b67c86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T19:11:49.837484Z",
     "start_time": "2025-04-07T19:11:49.834602Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to setup the vector database using ChromaDB\n",
    "def setup_vector_database(chunks, collection_name=\"cmu_student_guide\"):\n",
    "    # Create a ChromaDB client\n",
    "    chroma_client = chromadb.PersistentClient(\n",
    "        path=\"./chroma_db\",\n",
    "        settings=Settings(anonymized_telemetry=False)\n",
    "    )\n",
    "    \n",
    "    # Create a embedding function using SentenceTransformer\n",
    "    embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "        model_name=\"all-MiniLM-L6-v2\"\n",
    "    )\n",
    "\n",
    "    # Create collection or get existing one\n",
    "    collection = chroma_client.get_or_create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=embedding_func\n",
    "    )\n",
    "\n",
    "    # Add documents to the collection\n",
    "    for i in tqdm(range(0, len(chunks), 100), desc=\"Indexing documents\"):\n",
    "        batch = chunks[i:i + 100]\n",
    "        collection.add(\n",
    "            documents=[chunk['text'] for chunk in batch],\n",
    "            metadatas=[chunk['metadata'] for chunk in batch],\n",
    "            ids=[chunk['chunk_id'] for chunk in batch]\n",
    "        )\n",
    "\n",
    "    return collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "178d951321e582ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T19:11:49.845871Z",
     "start_time": "2025-04-07T19:11:49.843342Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to retrieve relevant chunks from the vector database for a given query\n",
    "def retrieve_relevant_chunks(collection, query, top_k=3):\n",
    "    # Query the collection for relevant chunks\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=top_k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    # Convert distances to similarity scores\n",
    "    results['scores'] = [1 - distance for distance in results['distances'][0]]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fac3d16e05e572fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T19:11:49.858687Z",
     "start_time": "2025-04-07T19:11:49.856089Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to generate an answer using OpenAI API based on the retrieved chunks and the query\n",
    "def generate_answer(openai_client, query, retrieved_chunks):\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"Source: {meta['title']}\\n{doc}\"\n",
    "        for doc, meta in zip(retrieved_chunks['documents'][0],\n",
    "                            retrieved_chunks['metadatas'][0])\n",
    "    ])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful CMU assistant. Answer based ONLY on this context:\n",
    "    {context}\n",
    "    Question: {query}\n",
    "    Answer concisely and cite sources. If unsure, say you I don't know.\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a factual CMU student assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dace1ee29ecb9f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T19:11:49.863543Z",
     "start_time": "2025-04-07T19:11:49.861397Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to query the CMU knowledge base\n",
    "def query_cmu_knowledge(collection, openai_client, question, top_k=3):\n",
    "    try:\n",
    "        retrieved_chunks = retrieve_relevant_chunks(collection, question, top_k)\n",
    "        answer = generate_answer(openai_client, question, retrieved_chunks)\n",
    "\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"sources\": retrieved_chunks['metadatas'][0]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Query failed: {str(e)}\")\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": \"Sorry, I couldn't process your question. Please contact The HUB.\",\n",
    "            \"sources\": []\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d3b902b8b75e970",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T19:11:49.871713Z",
     "start_time": "2025-04-07T19:11:49.868805Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to evaluate the generated answer using BERTScore\n",
    "def evaluate_response(generated_answer, reference_answer):\n",
    "    P, R, F1 = bert_score([generated_answer], [reference_answer], lang='en')\n",
    "    return {\n",
    "        \"bertscore_precision\": P.mean().item(),\n",
    "        \"bertscore_recall\": R.mean().item(),\n",
    "        \"bertscore_f1\": F1.mean().item()\n",
    "    }\n",
    "\n",
    "# Function to run the evaluation on a set of test cases\n",
    "def run_evaluation(collection, openai_client, test_cases):\n",
    "    results = []\n",
    "    for case in test_cases:\n",
    "        response = query_cmu_knowledge(collection, openai_client, case['question'])\n",
    "        metrics = evaluate_response(response['answer'], case['answer'])\n",
    "        results.append({\n",
    "            \"question\": case['question'],\n",
    "            \"generated_answer\": response['answer'],\n",
    "            \"reference_answer\": case['answer'],\n",
    "            **metrics\n",
    "        })\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fc35d359eb3613e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T19:11:54.910477Z",
     "start_time": "2025-04-07T19:11:49.877527Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|ââââââââââ| 10/10 [00:00<00:00, 37.86it/s]\n",
      "Chunking documents: 100%|ââââââââââ| 10/10 [00:00<00:00, 17239.23it/s]\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:01<00:00,  1.62s/it]/s]\n",
      "Indexing documents: 100%|ââââââââââ| 1/1 [00:01<00:00,  1.81s/it]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 46.51it/s]\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the deadline to add a course?\n",
      "A: I don't know.\n",
      "Sources:\n",
      "- cds-2024-c-first-time-first-year-freshman-admission-21feb2025\n",
      "- cds-2024-g-annual-expenses-21feb2025\n",
      "- cds-2024-e-academic-offerings-and-policies-21feb2025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 26.14it/s]\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             question generated_answer  \\\n",
      "0  How do I access library resources?    I don't know.   \n",
      "\n",
      "                            reference_answer  bertscore_precision  \\\n",
      "0  Use your Andrew ID at the library website             0.810589   \n",
      "\n",
      "   bertscore_recall  bertscore_f1  \n",
      "0           0.84631      0.828065  \n"
     ]
    }
   ],
   "source": [
    "# Setup the variables and run the code\n",
    "documents = load_pdf_documents(\"../data\")\n",
    "chunks = chunk_documents(documents)\n",
    "collection = setup_vector_database(chunks)\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "response = query_cmu_knowledge(\n",
    "    collection,\n",
    "    openai_client,\n",
    "    \"What is the deadline to add a course?\"\n",
    ")\n",
    "print(f\"Q: {response['question']}\")\n",
    "print(f\"A: {response['answer']}\")\n",
    "print(\"Sources:\")\n",
    "for source in response['sources']:\n",
    "    print(f\"- {source['title']}\")\n",
    "\n",
    "# Sample evaluation\n",
    "test_cases = [\n",
    "    {\n",
    "        \"question\": \"How do I access library resources?\",\n",
    "        \"answer\": \"Use your Andrew ID at the library website\"\n",
    "    }\n",
    "]\n",
    "evaluation_df = run_evaluation(collection, openai_client, test_cases)\n",
    "print(evaluation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276a1fcd16bbe72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T19:18:16.366209Z",
     "start_time": "2025-04-07T19:18:13.924365Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cathleen Kisak is a faculty member at Carnegie Mellon University, known for her role in the School of Computer Science. She has been involved in various educational and administrative capacities within the university. For specific information about her current role, research interests, or contributions, I recommend checking the official Carnegie Mellon University website or the School of Computer Science faculty directory for the most up-to-date information.\n"
     ]
    }
   ],
   "source": [
    "# Setup OpenAI API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=openai.api_key)\n",
    "\n",
    "# Function to get chat response from OpenAI API using the CMU-specific RAG model\n",
    "def get_chat_response(prompt, model_name=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    CMU-specific prompt model with:\n",
    "    - Structured academic responses\n",
    "    - CMU knowledge base context\n",
    "    - Error handling for student queries\n",
    "    \"\"\"\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"\"\"You are 'Andrew', the official CMU student assistant.\n",
    "                    Provide accurate information about Carnegie Mellon University including:\n",
    "                    - Academic policies\n",
    "                    - Course registration\n",
    "                    - Campus resources\n",
    "                    - Important deadlines\n",
    "                    Cite official sources when possible.\"\"\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"\"\"I couldn't access CMU information. Please:\n",
    "        1. Visit thehub.cmu.edu\n",
    "        2. Contact (412) 268-8186\n",
    "        Error: {str(e)}\"\"\"\n",
    "\n",
    "response = get_chat_response(\"Who is Cathleen Kisak?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
